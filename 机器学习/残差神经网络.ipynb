{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 残差神经网络综述\n",
    "\n",
    "AlexNet的提出开启了卷积神经网络应用的先河，随后的GoogleNet、VGG等网络使用了更小的卷积核并加大了深度，证明了卷积神经网络在处理图像问题方面具有更加好的性能；\n",
    "\n",
    "但是随着层数的不断加深，卷积神经网络也暴露出来许多问题：\n",
    "\n",
    "- 理论上讲，层数越多、模型越复杂，其性能就应该越好；但是实验证明随着层数的不断加深，性能反而有所下降。\n",
    "- 深度卷积网络往往存在着梯度消失/梯度爆炸的问题；由于梯度反向传播过程中，如果梯度都大于1，则每一层大于1的梯度会不断相乘，使梯度呈指数型增长；同理如果梯度都小于1，梯度则会逐渐趋于零；使得深度卷积网络难以训练。\n",
    "- 训练深层网络时会出现退化：随着网络深度的增加，准确率达到饱和，然后迅速退化。\n",
    "\n",
    "而ResNet提出的残差结构，则一定程度上缓解了模型退化和梯度消失问题：\n",
    "\n",
    "![](图片/残差神经网络图1.png)\n",
    "\n",
    "作者提出，在一个结构单元中，如果我们想要学习的映射本来是y=H(x)，那么跟学习y=F(x)+x这个映射是等效的；这样就将本来回归的目标函数H(x)转化为F(x)+x，即F(x) = H(x) - x，称之为残差。\n",
    "\n",
    "于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H(x)和x的差值，即去掉映射前后相同的主体部分，从而突出微小的变化，也能够将不同的特征层融合。而且y=F(x)+x在反向传播求导时，x项的导数恒为1这样也解决了梯度消失问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 残差神经网络详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 论文地址\n",
    "\n",
    "[《Deep Residual Learning for Image Recognition》](https://arxiv.org/pdf/1512.03385v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心思想\n",
    "将本来回归的目标函数H(x)转化为F(x)+x，即F(x) = H(x) - x，称之为残差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差单元\n",
    "ResNet的基本的残差单元如图所示：\n",
    "\n",
    "![](图片/残差神经网络图1.png)\n",
    "\n",
    "基本结构如图，假设每个单元输入的特征层为x，经过两个卷积层获得输出y，将x与y求和即得到了这个单元的输出；\n",
    "\n",
    "在训练时，我们将该单元目标映射（即要趋近的最优解）假设为F(x) + x，而输出为y+x，那么训练的目标就变成了使y趋近于F(x)。即去掉映射前后相同的主体部分x，从而突出微小的变化（残差）。\n",
    "\n",
    "用数学表达式表示为：\n",
    "\n",
    "$$ y = F(x,\\{W_i\\}) + W_sx $$\n",
    "\n",
    "其中：\n",
    "\n",
    "- x是残差单元的输入；\n",
    "- y是残差单元的输出；\n",
    "- F(x)是目标映射；\n",
    "- {Wi}是残差单元中的卷积层；\n",
    "- Ws是一个1x1卷积核大小的卷积，作用是给x降维或升维，从而与输出y大小一致（因为需要求和）；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-12T13:52:13.640733Z",
     "iopub.status.busy": "2020-06-12T13:52:13.640733Z",
     "iopub.status.idle": "2020-06-12T13:52:13.796725Z",
     "shell.execute_reply": "2020-06-12T13:52:13.794719Z",
     "shell.execute_reply.started": "2020-06-12T13:52:13.640733Z"
    }
   },
   "source": [
    "## 改进单元\n",
    "同时也可以进一步拓展残差结构：\n",
    "\n",
    "![](图片/残差神经网络图2.png)\n",
    "\n",
    "原论文中则以VGG为例：\n",
    "\n",
    "![](图片/残差神经网络图3.png)\n",
    "\n",
    "从VGG的19层，拓展到了34层。\n",
    "\n",
    "可见使用了残差单元可以大大加深卷积神经网络的深度，而且不会影响性能和训练速度.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-13T02:03:45.385826Z",
     "iopub.status.busy": "2020-06-13T02:03:45.385826Z",
     "iopub.status.idle": "2020-06-13T02:04:48.663075Z",
     "shell.execute_reply": "2020-06-13T02:04:48.662111Z",
     "shell.execute_reply.started": "2020-06-13T02:03:45.385826Z"
    }
   },
   "outputs": [],
   "source": [
    "# 先导入一堆的库\n",
    "import  tensorflow as tf\n",
    "from    tensorflow import keras\n",
    "from    tensorflow.keras import layers, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-13T02:40:10.385903Z",
     "iopub.status.busy": "2020-06-13T02:40:10.384897Z",
     "iopub.status.idle": "2020-06-13T02:40:10.435892Z",
     "shell.execute_reply": "2020-06-13T02:40:10.433883Z",
     "shell.execute_reply.started": "2020-06-13T02:40:10.385903Z"
    }
   },
   "outputs": [],
   "source": [
    "# 图形中，我们可以将这个神经网络分成不同的块，就是如下的\n",
    "\n",
    "\n",
    "class BasicBlock(layers.Layer):\n",
    "    \"\"\"这个继承自Layer的，是单独的一个Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__() #  调用父类的\n",
    "        \n",
    "        # 这个类有两个卷积\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()  # 批量规范化 该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1\n",
    "        self.relu = layers.Activation('relu')   # 激活函数\n",
    " \n",
    "        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    " \n",
    "        # 判断移动步长的\n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n",
    "            #self.downsample = layers.Conv2D(filter_num, (1,1), strides = stride)\n",
    "        else:\n",
    "            self.downsample = lambda x:x\n",
    "    def call(self, inputs, training=None):\n",
    " \n",
    "        # [b, h, w, c]\n",
    "        # 如下是构造残差神经网络，\n",
    "        out = self.conv1(inputs) # 先卷积\n",
    "        out = self.bn1(out)      # 然后批量规范化\n",
    "        out = self.relu(out)     # 然后激活函数\n",
    " \n",
    "        out = self.conv2(out)    # 再卷积\n",
    "        out = self.bn2(out)      # 再批量规范化\n",
    " \n",
    "        identity = self.downsample(inputs)  # 将input转成跟out相同的维度\n",
    " \n",
    "        output = layers.add([out, identity])  # 相加\n",
    "        output = tf.nn.relu(output)           # 激活函数\n",
    " \n",
    "        return output\n",
    "\n",
    "\n",
    "# 然后构造总体的残差神经网络\n",
    "\n",
    "class ResNet(keras.Model):\n",
    "    \"\"\"残差神经网络类，是一个模型\"\"\"\n",
    " \n",
    "    def __init__(self, layer_dims, num_classes=100): # [2, 2, 2, 2]\n",
    "        super(ResNet, self).__init__()\n",
    "        # 做一个\n",
    "        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n",
    "                                layers.BatchNormalization(),\n",
    "                                layers.Activation('relu'),\n",
    "                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n",
    "                                ])\n",
    " \n",
    "        self.layer1 = self.build_resblock(64,  layer_dims[0])\n",
    "        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    " \n",
    "        # output: [b, 512, h, w],\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(num_classes)\n",
    " \n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    " \n",
    "        res_blocks = Sequential()\n",
    "        # may down sample\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        # 每一块有多少个\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    " \n",
    "        return res_blocks\n",
    " \n",
    "    def call(self, inputs, training=None):\n",
    " \n",
    "        x = self.stem(inputs)\n",
    " \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    " \n",
    "        # [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # [b, 100]\n",
    "        x = self.fc(x)\n",
    " \n",
    "        return x\n",
    "def resnet18():\n",
    "    return ResNet([2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引用\n",
    "\n",
    ">[残差神经网络ResNet系列网络结构详解：从ResNet到DenseNet](https://blog.csdn.net/weixin_44936889/article/details/103774753)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
