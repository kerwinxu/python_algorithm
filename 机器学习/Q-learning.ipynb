{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习有**状态（state）、动作（action）、奖赏（reward）** 这三个要素。提到Q-learning，我们需要先了解Q的含义。Q为动作效用函数（action-utility function），用于评价在特定状态下采取某个动作的优劣。它是智能体的记忆。\n",
    "\n",
    "比如如下的Q-table,s是状态，a是动作，而q(a,s)是奖赏。\n",
    "\n",
    "Q-Table|a1|a2\n",
    "--|--|--\n",
    "s1|q(s1,a1)|q(s1,a2)\n",
    "s2|q(s2,a1)|q(s2,a2)\n",
    "s3|q(s3,a1)|q(s3,a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用一个小鸟飞行的例子\n",
    "取小鸟到下一组管子的水平距离和垂直距离差作为小鸟的状态。更准确地说  \n",
    "![](./图片/小鸟飞行图片.jpg)  \n",
    "\n",
    "对于每一个状态[公式]，[公式]为水平距离，[公式]为垂直距离。\n",
    "\n",
    "动作的选择\n",
    "每一帧，小鸟只有两种动作可选：\n",
    "- 向上飞一下。\n",
    "- 什么都不做。\n",
    "\n",
    "奖赏的选择\n",
    "小鸟活着时，每一帧给予1的奖赏；若死亡，则给予-1000的奖赏。\n",
    "\n",
    "## 训练 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "初始化 Q = {};\n",
    "while Q 未收敛：  \n",
    "    初始化小鸟的位置S，开始新一轮游戏    \n",
    "    while S != 死亡状态：  \n",
    "        使用策略π，获得动作a=π(S)   \n",
    "        使用动作a进行游戏，获得小鸟的新位置S',与奖励R(S,a)  \n",
    "        Q[S,A] ← (1-α)*Q[S,A] + α*(R(S,a) + γ* max Q[S',a]) // 更新Q  \n",
    "        S ← S' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中有些值得注意的地方：\n",
    "\n",
    "1. 使用策略π，获得动作a=π(S)   \n",
    "最直观易懂的策略π(S)是根据Q表格来选择效用最大的动作（若两个动作效用值一样，如初始时某位置处效用值都为0，那就选第一个动作）。  \n",
    "但这样的选择可能会使Q陷入局部最优：在位置  处，在第一次选择了动作1（飞）并获取了  的奖赏后，算法将永远无法对动作2（不飞）进行更新，即使动作2最终会给出  的奖赏。  \n",
    "改进的策略为ε-greedy方法：每个状态以ε的概率进行探索，此时将随机选取飞或不飞，而剩下的1-ε的概率则进行开发，即按上述方法，选取当前状态下效用值较大的动作。  \n",
    "2.更新Q表格Q表格将根据以下公式进行更新：\n",
    "$$Q(S,A)\\leftarrow (1-\\alpha)Q(S,A)+\\alpha[R(S,a)+\\gamma max Q(S'a)]$$\n",
    "\n",
    "- $\\alpha$ 为学习速率（learning rate）\n",
    "- $\\gamma$ 为折扣因子（discount factor）\n",
    "- $R$ 为眼前利益\n",
    "- $max Q(S'a)$ 为记忆中的利益，。它是小鸟记忆里，新位置[公式]能给出的最大效用值。如果小鸟在过去的游戏中于位置[公式]的某个动作上吃过甜头（例如选择了某个动作之后获得了50的奖赏），这个公式就可以让它提早地得知这个消息，以便使下回再通过位置[公式]时选择正确的动作继续进入这个吃甜头的位置[公式]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引用\n",
    ">[如何用简单例子讲解 Q - learning 的具体过程？](https://www.zhihu.com/question/26408259)  \n",
    ">[【强化学习】Q-Learning算法详解](https://blog.csdn.net/qq_30615903/article/details/80739243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
