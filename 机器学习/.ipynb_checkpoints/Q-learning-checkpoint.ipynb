{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习有**状态（state）、动作（action）、奖赏（reward）** 这三个要素。提到Q-learning，我们需要先了解Q的含义。Q为动作效用函数（action-utility function），用于评价在特定状态下采取某个动作的优劣。它是智能体的记忆。\n",
    "\n",
    "比如如下的Q-table,s是状态，a是动作，而q(a,s)是奖赏。\n",
    "\n",
    "Q-Table|a1|a2\n",
    "--|--|--\n",
    "s1|q(s1,a1)|q(s1,a2)\n",
    "s2|q(s2,a1)|q(s2,a2)\n",
    "s3|q(s3,a1)|q(s3,a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用一个小鸟飞行的例子\n",
    "取小鸟到下一组管子的水平距离和垂直距离差作为小鸟的状态。更准确地说  \n",
    "![](./图片/小鸟飞行图片.jpg)  \n",
    "\n",
    "对于每一个状态[公式]，[公式]为水平距离，[公式]为垂直距离。\n",
    "\n",
    "动作的选择\n",
    "每一帧，小鸟只有两种动作可选：\n",
    "- 向上飞一下。\n",
    "- 什么都不做。\n",
    "\n",
    "奖赏的选择\n",
    "小鸟活着时，每一帧给予1的奖赏；若死亡，则给予-1000的奖赏。\n",
    "\n",
    "## 训练 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "初始化 Q = {};\n",
    "while Q 未收敛：  \n",
    "    初始化小鸟的位置S，开始新一轮游戏    \n",
    "    while S != 死亡状态：  \n",
    "        使用策略π，获得动作a=π(S)   \n",
    "        使用动作a进行游戏，获得小鸟的新位置S',与奖励R(S,a)  \n",
    "        Q[S,A] ← (1-α)*Q[S,A] + α*(R(S,a) + γ* max Q[S',a']) // 更新Q  \n",
    "        S ← S' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中有些值得注意的地方：\n",
    "\n",
    "1. 使用策略π，获得动作a=π(S)   \n",
    "最直观易懂的策略π(S)是根据Q表格来选择效用最大的动作（若两个动作效用值一样，如初始时某位置处效用值都为0，那就选第一个动作）。  \n",
    "但这样的选择可能会使Q陷入局部最优：在位置  处，在第一次选择了动作1（飞）并获取了  的奖赏后，算法将永远无法对动作2（不飞）进行更新，即使动作2最终会给出  的奖赏。  \n",
    "改进的策略为ε-greedy方法：每个状态以ε的概率进行探索，此时将随机选取飞或不飞，而剩下的1-ε的概率则进行开发，即按上述方法，选取当前状态下效用值较大的动作。  \n",
    "2.更新Q表格Q表格将根据以下公式进行更新：\n",
    "$$Q(S,A)\\leftarrow (1-\\alpha)Q(S,A)+\\alpha[R(S,a)+\\gamma max Q(S'a')]$$\n",
    "\n",
    "- $\\alpha$ 为学习速率（learning rate）\n",
    "- $\\gamma$ 为折扣因子（discount factor）\n",
    "- $R$ 为眼前利益\n",
    "- $max Q(S'a)$ 为记忆中的利益，。它是小鸟记忆里，新位置[公式]能给出的最大效用值。如果小鸟在过去的游戏中于位置[公式]的某个动作上吃过甜头（例如选择了某个动作之后获得了50的奖赏），这个公式就可以让它提早地得知这个消息，以便使下回再通过位置[公式]时选择正确的动作继续进入这个吃甜头的位置[公式]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有这样的房间   \n",
    "![](./图片/Q-learning图片1.png)  \n",
    "\n",
    "这样看，我们可以将其进行建模  \n",
    "\n",
    "![](./图片/Q-learning图片2.png)   \n",
    "\n",
    "这就是房间对应的图。我们首先将agent（机器人）处于任何一个位置，让他自己走动，直到走到5房间，表示成功。为了能够走出去，我们将每个节点之间设置一定的权重，能够直接到达5的边设置为100，其他不能的设置为0，这样网络的图为：  \n",
    "\n",
    "![](./图片/Q-learning图片3.png)    \n",
    "\n",
    "Qlearning中，最重要的就是“状态”和“动作”，状态表示处于图中的哪个节点，比如2节点，3节点等等，而动作则表示从一个节点到另一个节点的操作。\n",
    "\n",
    "\n",
    "首先我们生成一个奖赏矩阵：  \n",
    "\n",
    "![](./图片/Q-learning图片4.png)\n",
    "\n",
    "1. -1表示不可以通过\n",
    "2. 0表示可以通过\n",
    "3. 100表示直接到达终点\n",
    "\n",
    "总结就是：R矩阵中非负的表示节点之间是可以相通的。\n",
    "\n",
    "同时，我们创建一个Q表，表示学习到的经验，与R表同阶，初始化为0矩阵。\n",
    "\n",
    "根据Q-learning转移方程：\n",
    "$$Q(s,a)=R(s,a) + \\gamma\\cdot \\underset{\\widetilde{a}}{max} \\{Q(\\widetilde{s},\\widetilde{a}) \\}$$\n",
    "\n",
    "- s表示当前的状态\n",
    "- a表示当前的动作\n",
    "- $\\widetilde{s}$ 表示下一个状态\n",
    "- $\\widetilde{a}$ 表示下一个动作\n",
    "- $\\gamma$ 表示贪婪因子，一般为0.8\n",
    "\n",
    "![](./图片/Q-learning图片5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-28T09:51:34.080177Z",
     "iopub.status.busy": "2020-05-28T09:51:34.079175Z",
     "iopub.status.idle": "2020-05-28T09:51:34.400989Z",
     "shell.execute_reply": "2020-05-28T09:51:34.399989Z",
     "shell.execute_reply.started": "2020-05-28T09:51:34.080177Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 建立 Q 表\n",
    "q = np.zeros((6, 6))\n",
    "q = np.matrix(q)\n",
    "\n",
    "# 建立 R 表\n",
    "\n",
    "r = np.array([[-1, -1, -1, -1, 0, -1], [-1, -1, -1, 0, -1, 100], [-1, -1, -1, 0, -1, -1], [-1, 0, 0, -1, 0, -1],\n",
    "              [0, -1, -1, 0, -1, 100], [-1, 0, -1, -1, 0, 100]])\n",
    "r = np.matrix(r)\n",
    "\n",
    "# 贪婪指数\n",
    "gamma = 0.8\n",
    "\n",
    "# 训练\n",
    "\n",
    "for i in range(1000):\n",
    "    # 对每一个训练,随机选择一种状态\n",
    "    state = random.randint(0, 5)\n",
    "    while state != 5:\n",
    "        # 选择r表中非负的值的动作\n",
    "        r_pos_action = []\n",
    "        for action in range(6):\n",
    "            if r[state, action] >= 0:\n",
    "                r_pos_action.append(action)\n",
    "        next_state = r_pos_action[random.randint(0, len(r_pos_action) - 1)]\n",
    "        q[state, next_state] = r[state, next_state] + gamma * q[next_state].max()\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "print(q)\n",
    "# 验证\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"第{}次验证\".format(i + 1))\n",
    "\n",
    "    state = random.randint(0, 5)\n",
    "    print('机器人处于{}'.format(state))\n",
    "    count = 0\n",
    "    while state != 5:\n",
    "        if count > 20:\n",
    "            print('fail')\n",
    "            break\n",
    "        # 选择最大的q_max\n",
    "        q_max = q[state].max()\n",
    "\n",
    "        q_max_action = []\n",
    "        for action in range(6):\n",
    "            if q[state, action] == q_max:\n",
    "                q_max_action.append(action)\n",
    "\n",
    "        next_state = q_max_action[random.randint(0, len(q_max_action) - 1)]\n",
    "        print(\"the robot goes to \" + str(next_state) + '.')\n",
    "        state = next_state\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引用\n",
    ">[如何用简单例子讲解 Q - learning 的具体过程？](https://www.zhihu.com/question/26408259)  \n",
    ">[【强化学习】Q-Learning算法详解](https://blog.csdn.net/qq_30615903/article/details/80739243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
