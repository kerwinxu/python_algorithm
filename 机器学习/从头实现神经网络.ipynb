{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现一个简单的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 砖块：神经元\n",
    "\n",
    "\n",
    "![](./图片/神经元砖块.jpg)\n",
    "\n",
    "这里做了3件事情，首先，每个输入都跟一个权重相乘（红色）\n",
    "\n",
    ". $ x_1 \\leftarrow x_1 \\times w_1$  \n",
    ". $ x_2 \\leftarrow x_2 \\times w_2$  \n",
    "\n",
    "然后，加权后的输入求和，在加上一个偏差b（绿色）：  \n",
    "$(x_1 \\times x_1) + (x_2 \\times w_2) +b$  \n",
    "最后这个结果传递给一个激活函数f  \n",
    "$ y= f(x_1 \\times x_1  + x_2 \\times w_2 +b ) $  \n",
    "激活函数的用途是将一个无边界的输入变成一个可预测的形式，常用的激活函数就是S型函数  \n",
    "![](./图片/激活函数s.jpg)  \n",
    "S型函数的值域是(0, 1)。简单来说，就是把(−∞, +∞)压缩到(0, 1) ，很大的负数约等于0，很大的正数约等于1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个简单的例子\n",
    "\n",
    "假设我们有一个神经元，激活函数就是S型函数，其参数如下：\n",
    "\n",
    ". $w=[0,1]$  \n",
    ". $b=4$  \n",
    "\n",
    "现在我们给这个神经元一个输入 $ x=[2,3]$, 我们用点积来表示。  \n",
    "\n",
    "$\\begin{array}{r} (w\\cdot x)+b =& (w_1\\times x_1+w_2\\times x_2)+b \\\\ =& 0*2+1*3+4 \\\\&7\\end{array}$  \n",
    "$y= f(w\\cdot x+b) = f(7) = 0.999$  \n",
    "\n",
    "当输入是[2, 3]时，这个神经元的输出是0.999。给定输入，得到输出的过程被称为前馈（feedforward）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编写一个神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-17T03:15:55.733296Z",
     "iopub.status.busy": "2020-06-17T03:15:55.733296Z",
     "iopub.status.idle": "2020-06-17T03:15:55.758290Z",
     "shell.execute_reply": "2020-06-17T03:15:55.757288Z",
     "shell.execute_reply.started": "2020-06-17T03:15:55.733296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights   # 权重\n",
    "    self.bias = bias         # 偏置\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias     # y = w*x+b\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把神经元组成网络\n",
    "\n",
    "所谓的神经网络就是一堆神经元。这就是一个简单的神经网络：\n",
    "\n",
    "![](./图片/简单神经元示例.jpg)  \n",
    "\n",
    "这个神经元有2个输入，有2个神经元的隐藏层，以及有一个神经元的输出层\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码神经网络：前馈\n",
    "\n",
    "![](./图片/简单神经元前馈网络.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-17T03:30:15.828980Z",
     "iopub.status.busy": "2020-06-17T03:30:15.827987Z",
     "iopub.status.idle": "2020-06-17T03:30:16.180641Z",
     "shell.execute_reply": "2020-06-17T03:30:16.178660Z",
     "shell.execute_reply.started": "2020-06-17T03:30:15.828980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ... code from previous section here\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    weights = np.array([0, 1])\n",
    "    bias = 0\n",
    "\n",
    "    # The Neuron class here is from the previous section\n",
    "    # 如下是做了3个神经元。\n",
    "    self.h1 = Neuron(weights, bias)\n",
    "    self.h2 = Neuron(weights, bias)\n",
    "    self.o1 = Neuron(weights, bias)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    # The inputs for o1 are the outputs from h1 and h2\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练神经网络，第1部分\n",
    "\n",
    "现在有这样的数据：\n",
    "\n",
    "![](./图片/简单神经网络训练数据1.jpg)  \n",
    "\n",
    "接下来我们用这个数据来训练神经网络的权重和截距项，从而可以根据身高体重预测性别：\n",
    "\n",
    "![](./图片/简单神经元前馈网络.jpg)\n",
    "\n",
    "我们用0和1分别表示男性（M）和女性（F），并对数值做了转化：\n",
    "\n",
    "![](./图片/简单神经网络训练数据1-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失\n",
    "\n",
    "训练网络之前，我们需要量化当前的网络是『好』还是『坏』，从而可以寻找更好的网络。这就是定义损失的目的。 \n",
    "\n",
    "我们在这里用平均方差（MSE）损失：$MSE = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_{true}- y_{pred})^2$ 让我们仔细看看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码：MSE损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-17T03:38:13.010532Z",
     "iopub.status.busy": "2020-06-17T03:38:13.009535Z",
     "iopub.status.idle": "2020-06-17T03:38:13.072529Z",
     "shell.execute_reply": "2020-06-17T03:38:13.070536Z",
     "shell.execute_reply.started": "2020-06-17T03:38:13.010532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络，第2部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-17T03:40:55.786045Z",
     "iopub.status.busy": "2020-06-17T03:40:55.785045Z",
     "iopub.status.idle": "2020-06-17T03:40:55.809042Z",
     "shell.execute_reply": "2020-06-17T03:40:55.807044Z",
     "shell.execute_reply.started": "2020-06-17T03:40:55.786045Z"
    }
   },
   "source": [
    "现在我们有了一个明确的目标：最小化神经网络的损失。通过调整网络的权重和截距项，我们可以改变其预测结果，但如何才能逐步地减少损失？\n",
    "\n",
    "损失看成是权重和截距项的函数。让我们给网络标上权重和截距项：\n",
    "\n",
    "![](./图片/简单神经元前馈网络.jpg)\n",
    "\n",
    "这样我们就可以把网络的损失表示为：  \n",
    "$L(w_1,w_2,w_3,w_4,w_5,w_6,b_1,b_2,b_3)$ \n",
    "\n",
    "如果我们要优化$w_1$,损失函数L会怎么变化，我们可以用 $\\frac{\\partial L}{\\partial w_1}$来表示，这个就是偏导\n",
    "\n",
    "我们计算一下\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} \\times \\frac{\\partial y_{pred}}{\\partial w_1}$\n",
    "\n",
    "反正我们最后将这个结果成\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$\n",
    "\n",
    "这个就是反向传播算法\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-17T04:02:53.515606Z",
     "iopub.status.busy": "2020-06-17T04:02:53.515606Z",
     "iopub.status.idle": "2020-06-17T04:02:53.938581Z",
     "shell.execute_reply": "2020-06-17T04:02:53.936607Z",
     "shell.execute_reply.started": "2020-06-17T04:02:53.515606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.142\n",
      "Epoch 100 loss: 0.017\n",
      "Epoch 200 loss: 0.008\n",
      "Epoch 300 loss: 0.005\n",
      "Epoch 400 loss: 0.004\n",
      "Epoch 500 loss: 0.003\n",
      "Epoch 600 loss: 0.003\n",
      "Epoch 700 loss: 0.002\n",
      "Epoch 800 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  # 导数啦，求偏导。\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # 权重，Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # 截距项，Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 100 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 引用\n",
    ">[用Python从头实现一个神经网络](https://zhuanlan.zhihu.com/p/58964140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
